\subsection{Methods} \label{s:cs:methods}



Generally, the clustering analysis can be split into three stages: 1) the data pre-processing 2) choosing the clustering model and 3) choosing the number of clusters. The first part is covered in the following section, \ref{s:cs:pre-processing}, while deciding on the clustering model and its configuration is presented in \cref{s:cs:right_config}.


\subsubsection{Pre-processing} \label{s:cs:pre-processing}

% gene filtering
The RNAseq data consists of a large number of genes, many of which are not uniformly expressed across the samples. In this section, a gene is considered unexpressed if more than 10\% of the samples have a value less than 1.5 TPM. Filtering out genes that meet these unexpressed conditions leaves the TCGA's tumour dataset with approximately 13,000 genes that are expressed in at least 90\% of the samples (approximately 370 samples). The threshold of 10\% is derived from the study by \citet{Robertson2017-mg}, where the authors removed all genes that had 'NA values more than 10\% across samples' (section \textit{Unsupervised mRNA expression clustering}). However, it is important to note that the authors of that study were using RSEM data, not TPMs.

From the resultant dataset, a subset of the most variably expressed genes is selected depending on the experiment. The most variably expressed genes are identified by the highest standard deviation to median ratio, which also takes into account the magnitude of the expression. This method of selection focuses on identifying the genes with the highest relative variability across the samples. This approach differs from that used by \citet{Robertson2017-mg}, which selects the top 25\% of genes based solely on standard deviation, without factoring in the magnitude of gene expression.


\subsubsection{Finding the right configuration} \label{s:cs:right_config}

In subsequent experiments, several clustering models were evaluated using the scikit-learn library \cite{Scikit-learn_undated-ax}, as introduced in \cref{s:lit:clustering}. The models tested included K-means, Wardâ€™s, Agglomerative Clustering, Birch, Gaussian Mixture, and Spectral Clustering. The performance of these models was assessed using clustering metrics such as Silhouette with cosine distance, Calinski-Harabasz, and Davies-Bouldin, detailed in \cref{s:lit:clustering_metrics}. Some methods were excluded from this thesis due to poor performance, typically forming only one or two groups despite various configurations.

The following adjustments based on the \citet{Scikit-learn_undated-ax} were made to the default settings of the models to optimise clustering outcomes:
\begin{itemize}
    \item \textbf{K-Means:} Maximum iterations increased to 1000.
    \item \textbf{Ward:} Employed agglomerative clustering with Ward linkage and a connectivity matrix computed using 5 nearest neighbours for each point.
    \item \textbf{Birch:} The threshold parameter `birch\_th` was set to 1.7
    \item \textbf{Gaussian Mixture:} The covariance type was set to diagonal with maximum iterations capped at 500.
    \item \textbf{Spectral Clustering:} Eigenvalue decomposition was performed using arpack, and the affinity matrix was based on nearest neighbours.
\end{itemize}


Before applying clustering methods, Principal Component Analysis (PCA) is utilized to reduce the data dimensions. This dimensionality reduction significantly enhances the performance of the clustering algorithms, as evidenced by the improved metric scores for PCA-transformed data compared to non-PCA data, shown in \cref{fig:cs:cs_metrics} and \cref{fig:ap:non_pca_metrics} in the Appendix. Specifically, the Silhouette score doubles, indicating a two-fold improvement in cluster tightness and separation. The Calinski-Harabasz score sees a threefold increase, reflecting better-defined clusters, while the Davies-Bouldin score halves, suggesting a better partition (lower is better). The number of PCA components is set to four, marking the threshold beyond which each additional component contributes less than 1\% to the overall PCA variance\footnote{The PCA variance is a measure of the information each component captures}.

The performance of the five clustering methods across $K\in(4, 13)$ is displayed in
\cref{fig:cs:cs_metrics}. It was empirically observed that the clustering method scored the best at K$\in{2,3}$, this is expected in a parse dataset like the gene expression of the MIBC where there are no clear defined groups. To avoid getting the best performance at K=2 or K=3, the lower bound was set to 4, while the upper limit at 13 as it was observed that the metrics decrease proportionally with the number of clusters. The number of 'X' markers on top of the bar marks the model's position in top three. From the plots, it can be seen that the K-means generally performs the best across the clustering methods. The inverse relation between the algorithm's performance over the number of groups is the most evident in the Calinski Harabasz score (\cref{fig:cs:cal_hab}) and in Davies Bouldin index (\cref{fig:cs:dav_boul}), but less pronounced in the Silhouette score (\cref{fig:cs:cosine}).

\begin{figure}[H]
    \captionsetup[subfigure]{justification=Centering}
    \centering
    \begin{subfigure}[!t]{0.85\textwidth}
        \includegraphics[width=\textwidth]{Sections/ClusteringAnalysis/Resources/cs_top3/PCA_top3_Silhoute_cosine.png}
        \caption{Silhouette using cosine distance}
        \label{fig:cs:cosine}
    \end{subfigure}
    \centering
    \begin{subfigure}[!t]{0.85\textwidth}
        \includegraphics[width=\textwidth]{Sections/ClusteringAnalysis/Resources/cs_top3/PCA_top3_Calinski_habrasz.png}
        \caption{Calinski Harabasz}
        \label{fig:cs:cal_hab}
    \end{subfigure}
    \centering
    \begin{subfigure}[!t]{0.85\textwidth}
        \includegraphics[width=\textwidth]{Sections/ClusteringAnalysis/Resources/cs_top3/PCA_top3_Davies_bouldin.png}
        \caption{Davies Bouldin}
        \label{fig:cs:dav_boul}
    \end{subfigure}
    \caption{The means of the three cluster metrics introduced in \cref{s:lit:clustering_metrics}: Silhouette (cosine), Calinski Harabasz and Davies Bouldin. The gene expression is processed according to \cref{s:cs:methods} and PCA with 5 components was applied. For each metric the top 3 most performing models are marked by "X". }
    \label{fig:cs:cs_metrics}
\end{figure}

To aid the decision making, the top 3 most performing configuration is displayed per cluster metric in \cref{fig:cs:cs_metrics_heatmap}. The two heat-maps are similar, each column represents the algorithm while the rows the cluster metrics, the colouring is done by the occurrence per algorithm. In the first heatmap, \cref{fig:cs:heatmap_gen} the colour is proportional with the occurrence of the model configuration, while in the second \cref{fig:cs:heatmap_cs} the colouring is given by the number of clusters. This means, that the first plot gives the most popular model configuration per algorithm, while the second the most frequent K. From this, K-Means with 5, Spectral Clustering with 7 clusters and K=5 and K=7 are the most frequent methods/group sizes. 

\begin{figure}[!htb]
    \captionsetup[subfigure]{justification=Centering}
    \centering
    \begin{subfigure}[!t]{0.65\textwidth}
        \includegraphics[width=\textwidth]{Sections/ClusteringAnalysis/Resources/cs_top3/top3_cs_gen_top3_heatmap_pca.png}
        \caption{Most frequent cluster model}
        \label{fig:cs:heatmap_gen}
    \end{subfigure}
    \centering
    \begin{subfigure}[!t]{0.65\textwidth}
        \includegraphics[width=\textwidth]{Sections/ClusteringAnalysis/Resources/cs_top3/top3_cs_size_top3_heatmap_pca.png}
        \caption{Most frequent K in top 3}
        \label{fig:cs:heatmap_cs}
    \end{subfigure}
    \caption{Most frequent model and K by the clustering metrics. For each heatmap the rows are given by top model/K for that metric, while the X axis contains the methods used. The \textbf{colouring is done per column}, so it can be determine the right configuration per model and for the number of clusters. The following shortcuts were used: \textbf{Wrd} - Ward, \textbf{Brch} - Birch, \textbf{GaussianMixture} - Gaussian Mixture Models, \textbf{Spec} - Spectral Clustering.}
    \label{fig:cs:cs_metrics_heatmap}
\end{figure}


Less common are the cluster models with high number of clusters such as Wrd\_9, Birch\_11 or GaussianMixture\_8. It can also be noticed that the cluster sizes oscillates between 5-7 groups which is in agreement with the other classification such as TCGA (5) and consensus (6) \citet{Robertson2017-mg,Kamoun2020-tj}.





\paragraph*{Silhouette distribution}

% Sillhouette
Hitherto, decision-making within this study was guided by the average values derived from clustering metrics. While this approach facilitated the initial selection of the most suitable model and the optimal number of clusters, it inherently neglected the variability inherent in the scores. To address this analytical shortfall, the distribution of the Silhouette scores\footnote{This experiment is an adapted version of the \href{https://tinyurl.com/sillhouete-distrib}{example} provided by the Scikit-learn library.} has been graphically represented in \cref{fig:cs:sill_distrib} for the K-means model, spanning 3 to 6 clusters. This constrained range was strategically selected to enhance visual interpretation and is underpinned by preceding results (\cref{fig:cs:cs_metrics}), which indicate marginal gains from augmenting the cluster count beyond this spectrum.


The first two rows of \cref{fig:cs:sill_distrib} display the subplots containing the Silhouette distribution and the 2-D scatter plot of the first two principal components. Generally, a Silhouette score above 0.5 is considered good, indicating that the sample is well-matched to its group. Scores around 0 suggest that the sample could belong to multiple clusters, while negative values denote mislabelling of the samples. Across the top four plots, it is apparent that the vast majority of samples fall below the 0.5 threshold, indicating a lack of clearly defined clusters and a high degree of data heterogeneity, a known characteristic of the MIBC 'molecular profile' (see \cref{s:lit:bladder_cancer}). Although K=3 has a higher mean score than other models, the top right plot shows that most samples still score below 0.5 also confirmed in \cref{fig:cs:sill_neg_above_th}.

% Comment on the bar plot and the number of samples
The bar plot in \cref{fig:cs:sill_neg_above_th} displays the number of samples on the Y-axis for each $K \in [3, 8]$ that either exhibit negative Silhouette scores (red) or are above the 0.5 threshold (green). Across all $K$ values, there are very few samples exceeding the recommended Silhouette threshold, indicating a lack of clearly defined clusters. For selecting the number of well-clustered samples, $K=5$ and $K=6$ show the highest counts of well-clustered samples, with $K=5$ also having an equal number of samples with negative Silhouette scores. The highest instances of mis-clustered samples occur with $K=2$ and $K=3$, which also have the fewest samples above the threshold. \Cref{fig:cs:sill_neg_above_th} suggests that K-means with $K=5$ and $K=6$ performs better than the others.


\begin{figure}[H]
    \captionsetup[subfigure]{justification=Centering}
    \centering
    \begin{subfigure}[!t]{0.49\textwidth}
        \includegraphics[width=\textwidth]{Sections/ClusteringAnalysis/Resources/cs_top3/sill_distrib/KMeans_3_sill_distrib.png}
        \caption{K=3 (Silhouette Mean: $0.43$)}
    \end{subfigure}
    \centering
    \begin{subfigure}[!t]{0.49\textwidth}
        \includegraphics[width=\textwidth]{Sections/ClusteringAnalysis/Resources/cs_top3/sill_distrib/KMeans_4_sill_distrib.png}
        \caption{K=4 (Mean: $0.37$)}
    \end{subfigure}
    \centering
    \begin{subfigure}[!t]{0.49\textwidth}
        \includegraphics[width=\textwidth]{Sections/ClusteringAnalysis/Resources/cs_top3/sill_distrib/KMeans_5_sill_distrib.png}
        \caption{K=5 (Mean: $0.40$)}
    \end{subfigure}
    \centering
    \begin{subfigure}[!t]{0.49\textwidth}
        \includegraphics[width=\textwidth]{Sections/ClusteringAnalysis/Resources/cs_top3/sill_distrib/KMeans_6_sill_distrib.png}
        \caption{K=6 (Mean: $0.35$)}
    \end{subfigure}
    \centering
    \caption{The silhouette distribution of the K-means model for K ranging from 3 to 6. The scatter plot on the right plots the first 2 components of PCA (4 components) coloured by the cluster labels from the method. A good silhouette score is considered to be above 0.5, which is marked on by the blue dotted line. The red dotted line represents the mean value. The values in the brackets represent the Mean Silhouette values across the samples.}
    \label{fig:cs:sill_distrib}
\end{figure}


Lastly, the Sankey plot visualises the cluster evolution as $K$ increases. Initially, at $K=3$, three groups are identified: Basal, Luminal, and Luminal Infiltrated. As $K$ increases to 4, the Basal group divides into four subgroups, which then consolidate into three at $K=5$; these subgroups are maintained through $K=6$. Surprisingly, the large Luminal group only splits at $K=6$, which deviates from previous classifications where the Luminal group typically divides into smaller subgroups earlier than the Basal.

Based on the analysis conducted in this section, $K=5$ and $K=6$ emerge as the most suitable configurations for the model. Specifically, $K=5$ consistently ranks among the top three best-performing models as evidenced in \cref{s:lit:clustering_metrics} and \cref{fig:cs:cs_metrics_heatmap}. Given the clustering metrics, $K=5$ was selected over $K=6$ as the optimal configuration moving forward.


\begin{figure}[H]
    \centering
    \begin{subfigure}[H]{0.8\textwidth}
        \includegraphics[width=\textwidth, keepaspectratio]{Sections/ClusteringAnalysis/Resources/cs_top3/sill_distrib/sill_neg_above_th.png}
        \label{fig:cs:sill_neg_above_th}
        \caption{Number of samples with negative and above 0.5 Silhouette scores}
    \end{subfigure}
    \centering
     \begin{subfigure}[!t]{0.8\textwidth}
        \includegraphics[width=\textwidth, keepaspectratio]{Sections/ClusteringAnalysis/Resources/cs_top3/sill_distrib/sky_kMeans.png}
        \caption{Comparison across clustering methods}
    \end{subfigure}
    \centering
    \caption{The bar plot presents the numbers samples that are above a 'good' Silhouette threshold vs the samples with negative values. The sankey plot shows the comparison across the different configurations of K-Means.}
    \label{fig:cs:sankey_comp}
\end{figure}




\subsubsection{Pipeline}

The pipeline and configuration of the different components can be summarised in the \cref{fig:cs:clustering_pipeline} and has three stages:
\begin{enumerate}
    \item The data is pre-processed, only the genes that are expressed in 90\% of the samples. \item From this, the top $3500$ with the highest std/median ratio are kept. Ensuring to retain the genes that have a high relative variance across samples. The number of genes represent ~25\% of the filtered dataset and it is a similar size to the data used by \citet{Robertson2017-mg}.
    \item To the resultant gene set of $408x3500$ (samples, genes) is log2(TPM+1) transformed (account for $log2$ 0 values) and the PCA with 4 components is applied
    \item On the $408x4$ K-means with K=5 is applied, resulting in the scatter plot at the end of the pipeline.
\end{enumerate}

The clustering analysis established in this part of the thesis is simple using basic clustering and dimension reduction techniques. So far the approach taken was unbiased and the configurations were decided based on established clustering metrics. There was no domain knowledge used, apart from the guiding parameters for the numbers of genes to be kept from the TCGA's MIBC paper from \citet{Robertson2017-mg}. It can be noted that the number of clusters arrived is the same as in the above-mentioned work and that the consensus \citet{Kamoun2020-tj} found 6 subtypes. 

In the following section, the 5 clusters are explored in more depth and put in the context of the other work performed in the Jack Birch Unit \citet{Baker2022-bj}, TCGA \citet{Robertson2017-mg}, Lund classifier \citet{Marzouka2018-ge} and the consensus \citet{Kamoun2020-tj}. It will also cover their clinical implication by performing Kaplan-Meier survival analysis and Defferentialy Expressed Analysis (DEA).


% \begin{landscape}
% \thispagestyle{empty} % Optional: to remove the page number if desired
% \begin{figure}[!htb]
%     \includegraphics[height=1\textheight, width=1.0\textwidth,keepaspectratio]{Sections/ClusteringAnalysis/Resources/clustering_pipeline.png}
%     \caption{Summary of the clustering analysis developed in this section}
%     \label{fig:cs:clustering_pipeline}
% \end{figure}
% \end{landscape}


% \begin{figure}[H]    
% \centering
% \includegraphics[height=0.9\textheight,keepaspectratio]{Sections/ClusteringAnalysis/Resources/clustering_pipeline_rotated.png}
%     \caption{\adjustbox{angle=90}{Summary of the clustering analysis developed in this section}}
%     \label{fig:cs:clustering_pipeline}
% \end{figure}

\begin{figure}[H]
    \centering
    \begin{minipage}[b]{0.75\textwidth}
        \centering
    \includegraphics[height=\textheight,keepaspectratio]{Sections/ClusteringAnalysis/Resources/clustering_pipeline.png}
    \end{minipage}%
    \begin{minipage}[b]{0.2\textwidth}
        \centering
        \rotatebox{90}{\begin{minipage}{\textheight}\caption{Summary of the clustering analysis developed in this section}\end{minipage}}
    \end{minipage}
    \label{fig:cs:clustering_pipeline}
\end{figure}