
\section{Methods for analysing omics data} \label{s:lit:multi-omics}

\vspace{3mm}
% \noindent\rule{17cm}{0.2pt}
\fbox {
    \parbox{\linewidth}{
      \begin{itemize}
        \item Overview of the methods used in analysing gene expression and mutation data
        \item Integrative approaches
        \item iClusters and Deep Learning methods
      \end{itemize}
    }
}
\vspace{3mm}

% One of the largest tumour cohort is \acrshort{tcga}\cite{Tcga2018-sj} which consists of roughly 20000 samples across 33 cancer types. An impressive effort, but if one focuses on particular cancer, the available data becomes considerable smaller. For example, the data available for \acrfull{mibc} cancer is 408 samples with a wide-range of molecular information available gene expressions, somatic mutations, copy number variations and metadata \cite{Robertson2017-mg}. This then, becomes a situation of a problem with a low number samples set and large number of features; there are more than 56k genes in a sample, which is usually the other way in the problems addressed in typical ML problems.


% Problems if we just use dim reductions or if we don't take into consideration all aspects of the genesThe stratification of muscle-invasive bladder cancer based on gene expression is characterised by a small number of samples and a large number of features. 
Previous approaches, such as those included in the MIBC consensus \citep{Kamoun2020-tj}, address the problem of high feature dimensionality by filtering out unexpressed genes by applying a dimension reduction algorithm, on which then cluster analysis is conducted. This process makes a significant assumption due to the reliance on dimension reduction algorithms, which tend to select data points with the highest variance. From an information theory perspective, this assumption holds relevant information typically indicated by high variance\footnote{That is, the more different a data point is from the average, the more information it contains. One can think of this as, if you were to live in the UK, you would be more interested when  sunny days are, not when it rains.}. However, genes that commonly drive cancer, such as \textit{TP53}â€”which controls cell cycle checkpoints and DNA integrity before cell division - may exhibit high average expression levels but not a high variance. Analysis of TCGA data for bladder cancer revealed that \textit{TP53} expression remains consistent across samples, resulting in low variance \citep{Robertson2017-mg}. However, the gene also suffers mutations in bladder samples, which have an impact on the protein produced and then on the tissue; see \cref{fig:lit:2020_consens}. An important paralog to \textit{TP53} is \textit{TP63} which is not as frequently mutated in the Urothelial Carcinoma but it expression has been linked to basal subtypes \citep{Choi2012-kk,Karni-Schmidt2011-ps,Choi2014-ed}.

Such instances pose a remarkable challenge, as the model must avoid dismissing genes that do not have a large variance. The implications of choosing a different gene selection strategy compared to the standard are discussed in \cref{s:cs:agg_filt}. In addition, the project encompasses extensive analysis of various datasets, pursuing objectives beyond disease stratification. Reviewing advances in the analysis of biological data from different perspectives informs the selection of the most appropriate methods for the project's aims.


% Later, we will see that this can be measured by the exclusive mutations/expression related to the other genes in the set.
The previous work done in the field can be categorised based on the data given to the models. It starts with a review of the research conducted on analysing gene expression data in \ref{s:lit:rnaSeq}, followed by efforts to find the relevant mutant genes in \ref{s:lit:mutations} and then on integrative approaches with a focus on iCluster \ref{s:lit:multi-view}. The section briefly touches on genomics work, which uses machine learning (\ref{s:lit:dl_genomics}) and concludes with a critical discussion on which method is the most suitable for the project.

\subsection{Gene expression data} \label{s:lit:rnaSeq}

\vspace{3mm}
% \noindent\rule{17cm}{0.2pt}
\fbox {
    \parbox{\linewidth}{
      \begin{itemize}
        \item Challenges in working with gene expression
        \item Prevalent approach is hierarchical clustering on RNAseq data
        \item Other clustering techniques
      \end{itemize}
    }
}
\vspace{3mm}

% Talk about how and why unsupervised learning is used for RNAseq.
% Talk about the consensus in the 2020 paper
Proteins are the building blocks of every living organism and implicitly of tissues. To understand the malfunctioning of the biological processes that cause cancer, protein data (proteomics) are the ideal data to analyse malignant behaviour. However, measuring proteins in samples is difficult and expensive. As an alternative, RNAseq data is used, which is also increasingly more affordable. It is worth mentioning that this measures the abundance (or transcript) of a gene\footnote{Remember the analogy of the RAM at the start of this Chapter.  DNA is the instruction from the hard drive, where RNA is the instruction from memory and protein is what the instruction creates.} it is not a direct measure of proteins and it might have biological limitations. From an engineering perspective, any additional step in the process is a potential risk of adding more errors to the output. Similarly, RNAseq may not always be an accurate representation of biological processes. This can be overcome by domain knowledge in the field, and it is here that collaboration with JBU plays a vital role.

Another aspect of the available datasets is that the samples usually come from tumours because obtaining a sample involves surgery, hence there is less publicly available data on healthy or non-tumour tissue. In contrast, as JBU focusses on bladder tissue, there is data collected on the urothelium that is the normal tissue counterpart of the cell that is usually transformed in urothelial cancer.

The most popular method for cluster analysis is the hierarchical clustering used in TCGA work \citep{Robertson2017-mg}, consensus \citep{Kamoun2020-tj} and Lund \citep{Sjodahl2017-xr}, explained in detail in \cref{s:lit:subtypes_mibc}; the method is covered in \cref{s:lit:clustering}. In literature, this method is preferred over the more standard K-means clustering as it can find subtypes of varying sizes as opposed to similar sizes found using K-means. Nevertheless, it is still a parametric solution where the user/researcher needs to make a choice on the dendrogram cut.

% Talk about sci-fold
More recent work is from \citet{Tian2021-vu} who used the Stiefel manifold method on gene expression, miRNA expression data, and DNA methylation. Compared to previous work discussed in this subsection, \citet{Tian2021-vu} focus on four other cohorts of TCGA: glioblastoma multiforme (GBM), breast invasive carcinoma (BIC) Skin Cutaneous Melanoma (SKCM) and Acute Myeloid Leukemia (AML). In their work, the authors suggest that using both gene expression and DNA methylation yields better subtyping, but they do not provide clear results on their model.

% other approaches 
Gene microarray data is an older method for obtaining genomic information from tissue which, compared to RNAseq, is less sensitive to differences in gene abundance, unable to detect highly or low-expression genes \citep{Wang2009-lj}. Some studies take multiple measures of the biopsy, adding a dimension of time to the data and making it suitable for a special type of \acrfull{ann}, \acrfull{snn}. \citet{Capecci2020-uj} combined SNN with \acrfull{ea} to process microarray data to classify gene interactions based on their temporal feature. The study focusses on dermatitis, a skin condition that enables multiple biopsies that give the temporal dimension. This type of sampling is however not possible for most of the diseases, including MIBC.

% Introducing neuroevolution
By combining ANN and EA, \citet{Grisci2019-xn} have introduced a neuroevolution model for the analysis of microarrays. Their research is built on the FS-NEAT model \citep{Whiteson2005-dn} which is an idiosyncratic version of the NEAT model (Neural Networks through Augmenting Topologies) focused on feature selection (FS) \citep{Stanley2002-tg}. The central idea of the NEAT family is to use evolutionary methods to find the optimal initial network structure, which can then be used for classification of gene expression. However, this method required the known classes for the samples, which can then be assigned to the gene patterns found by the work of \citep{Grisci2019-xn}

\subsection{Gene Mutations} \label{s:lit:mutations}

\vspace{3mm}
\fbox {
    \parbox{\linewidth}{
      \begin{itemize}
        \item Identifying the driver genes is formalised as finding a submatrix of relevant genes 
        \item Dendrix \citep{Vandin2012-cf} from which other work has been derived
        \item MDPFinder \citep{Zhao2012-wj} uses EA to find the submatrix of relevant genes
      \end{itemize}
    }
}
\vspace{3mm}


Gene mutations are anomalies that occur at the gene level within the DNA, where one (or more) base (nucleotides) is replaced by the other bases (3 possibilities) or insertion/deletion can happen to the gene. Depending on where they occur in the gene sequence, mutations have more or less a negative effect on protein function. Mutations that contribute to tumorigenesis are called driver genes and those that have a neutral contribution are known as passenger mutations \citep{Ciriello2012-hi}. To retrieve mutated genes, the \acrlong{wes} technique is used to target portions of the gene and compare it with the reference genome \citep{Schneider2016-ml}. This section covers computational models that seek to find the driver gene in a tissue, which highlights the standard methods to processing the mutation data. This type of analysis is performed separately from the gene expression clustering, whereas the project aim is to integrate the two analysis at one computational stage.

%  dimension reduction, gene subsets, finding the submatrix 
\citet{Vandin2012-cf} determined gene mutation exclusivity by using matrices and demonstrated that finding the subset (or submatrix) is an NP-hard problem\footnote{NP stands for Non-deterministic polynomial time and computer scientists describe the NP problem as the most complex problems in terms of the computational cost.}. The selection process of the relevant genes has been demonstrated by the authors to be NP-hard, meaning that it takes a lot of computational resources to solve it due to its complexity. Dendrix, the introduced model, constructs a submatrix with the driver genes of the disease from a larger set of genes. Specifically, it looks at finding the mutated genes that have the following two properties: high coverage (common) and high exclusivity (specific to some samples). The genes that need to be included in the submatrix are chosen by a weighting factor, which takes into account the above two properties as well as the Pearson correlation of the gene expression. Dendrix provides two solutions: a greedy algorithm (requiring a large dataset) and a stochastic one, Monte Carlo. Importantly, Dendrix is a \textit{de novo} algorithm that does not need \textit{a priori} data on gene interactions to determine their significance, and is mathematically rigorous. The authors applied the algorithm to lung adenocarcinoma and multiforme glioblastoma and found known pathways such as \textit{p53} or \textit{Rb pathways}.


% Introducing Comet 
Building on the work of \citeauthor{Vandin2012-cf}, there are several variations of the Dendrix model proposed \citep{Leiserson2013-da,Szczurek2014-dh,Leiserson2015-yk}. Noteworthy is the COMET \citep{Leiserson2015-yk} model developed in collaboration with Vandin to overcome the bias introduced by highly mutated genes to computational models derived from Dendrix \citep{Vandin2012-ns}. \cite{Leiserson2015-yk} accomplished this by introducing a scoring mechanism that measures how exclusive a mutation is to cancer. This is a more advanced version of the weighting function of the original paper by \citet{Vandin2012-cf} which was used to search for the submatrix of the driver genes. They compared the results with different models\footnote{Multi-Dendrix \citep{Leiserson2013-da} created by the same group and are variations of the Dendrix model, \citet{Vandin2012-cf}. The other model that they compare with is mutex \citep{Babur2015-qk} which also looks at the exclusivity of mutations.} using TCGA data sets and COMET outperforms the other models \citep{Leiserson2013-da,Szczurek2014-dh} in terms of finding the genes predominantly mutated in that particular cancer.

% Introducing MDPFinder 
%  The novelty of Zhao et al. is that they take the problem defined by Vadin et al. and replaced the Monte Carlo algorithm with the two other algorithm
\citet{Zhao2012-wj} took a different approach to the submatrix problem formalised by \citet{Vandin2012-cf}. Through their \textit{de novo} model, MDPFinder \citep{Zhao2012-wj}, the authors proposed two solutions to finding the gene mutations of the driver. The first one is an optimisation algorithm that finds the submatrix of the relevant genes in the carcinogenesis, a method guaranteed to find the solution, but highly dependent on the given data. Importantly, \citet{Zhao2012-wj} found that despite the submatrix problem being an NP-hard problem \citep{Vandin2012-cf} it can be approximated relatively fast. The second solution proposed was a simple form of \acrlong{ea} which enables a wider search space. In EA terminology, MDPFinder encodes individuals (potential genes) as strings (of length n, where n is the number of genes). One can see this solution as a method for the column selection method applicable to any dataset. The novelty in the evolutionary model comes from the fitness function, which accounts for both the mutation information (through the rank of the matrix) and gene expression (via Pearson correlation). This model is applied to multiple datasets (from both TCGA and those used with Dendrix \citet{Vandin2012-cf}) and all models\footnote{Remember that \citet{Vandin2012-cf} proposed two algorithms, the stochastic and greedy algorithm} yield the same results, but \citet{Zhao2012-wj} have a faster computational time\footnote{The scale is in seconds}. It is worth emphasising that \citet{Zhao2012-wj} uses \acrshort{ea} for their ability to search widely the solution space rather than the transparency of the method. The authors applied the algorithm to head and neck squamous cell carcinoma, glioblastoma, and ovarian cancer, and revealed subsets of gens with biological functions.

MutSigCV \citep{Lawrence2013-pl} is a method used to identify driver genes from whole exome sequencing (\acrshort{wes}) data. Calculate the probability that a gene is mutated by chance considering factors such as mutation rate across the cohort and mutation types. The statistical model is designed to identify a subset of driver genes, even in the presence of many mutations in the disease. MutSigCV was used in the MIBC consensus study \citep{Kamoun2020-tj} to analyse mutation data.

There is no consensus on how the mutation data is processed, as in the case of RNASeq, where hierarchical clustering is the canonical approach. However, it seems that in these datasets, a more comprehensive range of algorithms was applied with the work of \citet{Zhao2012-wj} (MDPFinder), which can be seen as an early effort to apply \acrshort{ea} to genomics. In addition, \citet{Vandin2012-cf} mathematically formalise the search for driver mutations, which can help the computational side. MDPFinder and Dendrix prove to be two other relevant models for analysing mutations. 

Whereas most of the research efforts presented here differ in terms of aims from the current project, \citet{Zhao2012-wj} share some similarities. Instead of basing the method on gene expression, \citet{Zhao2012-wj} first analyses the mutations and then uses gene expression. In this thesis, a representation is first built on the gene expression, and then the mutation information is added. The network approach presented in this thesis offers a more robust option for integrating other data types such as transcription factors.


\subsection{Integrative approaches} \label{s:lit:multi-view}

\vspace{3mm}
\fbox {
    \parbox{\linewidth}{
      \begin{itemize}
        \item Graph/Network theory is the prevalent method
        \item iCluster used for data integration based on NMF 
        \item There are some efforts to integrate multiple steps in a pipeline
      \end{itemize}
    }
}
\vspace{3mm}

% introducing graphs and how they may be used
In previous sections, mutation data is analysed with the central goal to find the submatrix that represents the driver genes. It can be stated that it represents a linear algebraic method to genetics, but it is not the only one present. Analysing the data through the lens of a network/graph problem is a widely used method to understand the gene relationship. The end goal of genomic data analysis is to find and understand the interactions in the gene network and its perturbation in healthy tissue, as this is more likely to lead to treatment solutions. Specifically, researchers are interested in the mutated genes that influence the network. Therefore, graph theory is essential for processing this data. As presented in \cref{s:lit:graph_overview}, in the context of genomic data, nodes are represented by genes, and edges are the connections between them. The graph theory aims to highlight the nodes with important connections that drive the network, and the importance of a connection (edge) is represented by a weighting factor.

% MEMO
MeMo (Mutual exclusivity model - \cite{Ciriello2012-hi}) is a graph model that uses networks and community detection to find the exclusive groups of mutated genes, as well as different statistical methods. The model integrates gene mutations, copy number alterations and expression into a network representation which is then analysed to find similar groups of genes. Their approach is familiar to the research efforts covered in \cref{s:lit:mutations} as the authors first build a binary matrix based on the altered genes, followed by a graph representation which is then correlated with some pathway information. Finally, an exclusivity algorithm is applied with aims similar to those of the Dendrix \citep{Vandin2012-cf}. \citet{Ciriello2012-hi} applied the model to the glioblastoma and breast cancer TCGA cohorts and were able to validate against known mutated genes such as \textit{TP53} but also showed potential new biology. MeMo uses the Jaccard coefficient to build the network from mutation compared to the correlation metrics from gene expression used in the networks approaches explored in this project. 

% DriverNet and DawnRank.
DriverNet \citep{Bashashati2012-lk} creates a bipartite graph using combined transcriptomic, mutation, and information. The network is created on information known from protein-protein interaction networks to which multiple data types are applied using an influence graph algorithm developed by \citet{Vandin2011-bs}\footnote{Note this is the same first author who developed the Dendrix model and the work is presented in \cref{s:lit:mutations}}. DriverNet processes these edges through a greedy optimisation approach to find potential driver genes based on the connection between the two layers of information. The work is applied to four cancers and some of the known important genes, such as \textit{ERBB2} and \textit{EGFR}, are detected in glioblastoma and breast cancer. The research effort of \citet{Bashashati2012-lk} shows another network method that can enable the integration of multiple datasets.

DriverNet was developed by \citet{Hou2014-se} who also introduced DawnRank, a model that helps identify driver genes in a sample and not across a population, as previous models. The authors combine DriverNet with PageRank to find driver genes; this is the famous algorithm upon which Google was built. \citet{Hou2014-se} build the gene interaction network with MeMo \citep{Ciriello2012-hi} which uses mutations, expression, and copy number alterations data. DawnRank uses both tumorous and healthy samples from the donor, which enables scientists to find individual genetic differences specific to the tumour, not the individual. \citet{Hou2014-se} consider that DawnRank's main limitations are that it depends on building the molecular interaction network, which is usually incomplete and is neither cancer nor patient specific. 


Finding how genes are related to each other and if they are part of a similar pathway is a common analysis in genetics. The most popular method is \gls{GSEA} which receives a list of genes and then performs a wide search. There are other methods with similar goals GANPA (Gene Association Network-based Pathway Analysis - \citep{Fang2012-vr}), LEGO (Link Enrichment of Gene Ontology \citep{Dong2016-zs}) or the work of \citet{Cava2018-rv}. Network representation is important because it helps to determine the distance or proximity of genes on a pathway. These can also be seen as a more advanced method to determine the central (or most important) nodes in a network, but relies on the gene pathway information, which is incomplete.

In the work of \citep{Xie2021-al} was to quantify and review the work done in the field of Gene Set Analysis. They reviewed 504 papers and looked at their performance based on citations, how they correlate with different approaches, and how the models have been benchmarked. In their review, the authors proposed a set of criteria that increases the quality of bioinformatics research, but also suggested embedding more domain knowledge. Most of them are similar to the guiding principles in Machine Learning, defined by using benchmarks, publishing the code as open-source, and using open-access datasets.

% LEGO, GANPA and Cava et al.
% When the gene network interaction is seen as a graph, the connections of a gene to the other is relevant as well as the individual weight of each edge. Driver genes may be found by the number and strength of its connections. Finding this is formally known in graph theory as analysing the degree of centrality of a node. Solutions to this problem are GANPA ("Gene Association Network-based Pathway Analysis") \cite{Fang2012-vr} and LEGO ("functional Link Enrichment of Gene Ontology or gene sets") \cite{Dong2016-zs} which aim to find the relevant genes in the network. The main difference between these two is that \citet{Fang2012-vr}propose a solution to a problem classified as Functional Category Score (FCS) while \cite{Dong2016-zs} solve Over Representation Analysis (ORA). 

% \citet{Cava2018-rv} build on LEGO/GANPA to integrate multiplex networks with the goal of the network drivers in pan-cancer. The authors compare their model with some already mentioned above MDPFinder \cite{Zhao2012-wj}, Dendrix\cite{Vandin2012-cf}, DriverNet \cite{Bashashati2012-lk}, DawnRank \cite{Hou2014-se} and others (see tables from Appendix \cref{ap:tables_models}). It can be noticed that all three models LEGO, GANPA and \citet{Cava2018-rv} are using pathway, networks and expression information but not mutations.

% "Gene Association Network-based Pathway Analysis" (GANPA) \cite{Fang2012-vr} is a method that determines the weighting in signalling pathways by considering the microarrays data. A similar approach was developed by Dong et al. \cite{Dong2016-zs} called LEGO ("functional Link Enrichment of Gene Ontology or gene sets"). The main difference between these two is that Fang et al. propose a solution to a problem classified as Functional Category Score (FCS) while Dong et al. solve Over Representation Analysis (ORA). The actual difference seems to be related to the gene of interest but I don't understand it as I haven't considered it important to follow up. However, the importance of these two models comes from their common graph approach and that is of analysing the degree of centrality of a gene. This means how many connections the genes have and what's their weight. Processing this gives information about how relevant can be.

% GANPA/LEGO is using data on gene expression (microarrays) or network pathways (KEGG). 
% Cava et al. \cite{Cava2018-rv} builds on LEGO/GANPA to integrate multiplex networks with the goal of the network drivers in pan-cancer. 


\subsection*{Pipelines to integrate multiple datasets} \label{s:pipelines}


% Related to GANPA and Cava - GeneSetCluster
Within the same class as GSEA, GANPA, and LEGO \citep{Fan2020-yb, Dong2016-zs}, \citet{Ewing2020-os} introduced a pipeline (GeneSetCluster) to process the gene sets produced by different sequencing techniques. GeneSetCluster consists of three stages: pre-processing (harmonise in the paper), computing the distances between genes and clustering (k-means and hierarchical clustering). The authors do not propose a new integrative method to find potentially relevant genes in the dataset, but propose a general pipeline solution that groups the output of gene set analysis (e.g., GSEA) for multiple data types.

% Multi-genetics approach, something built on Neuroevolution
\citet{Feltes2020-bz} perform a thorough pan-cancer analysis of gene expression in a large number of datasets. After the initial preprocessing stage, the authors reduced it to only 82 microarray studies and 17 RNAseq datasets. They applied the Neuroevolution model \citep{Grisci2019-xn} which was built on FS-NEAT \citep{Whiteson2005-dn}, the model covered in \cref{s:lit:rnaSeq}. The last step was to use STRING 11 \citep{Szklarczyk2019-pu} metasearch to build a Protein-Protein network. An interesting conclusion from the study of \citet{Feltes2020-bz} was that the underexpressed genes in the bladder are similar to those of breast, colorectal, lung, and ovarian cancer. Moreover, the study suggests that there is a stronger correlation between the carcinogenic character of over-expressed genes compared to an under-expressed one. This further demonstrates that integrative approaches form a better representation of biology.

% iPAC
\citet{Aure2013-je} introduce multiple statistical tools that correlate copy number alterations with gene expression through a pipeline called iPAC (in-trans process associated and ciscorrelated) and further refined their results through Gene Ontology \citep{Thomas2022-kn}. The authors applied their method to breast cancer, which is capable of finding known driver genes but potential candidates. iPAC is a statistical method that focusses on disruption of the epigenetic machinery with potential applications to MIBC, but it does not consider other relevant information such as gene expression or mutations in the computational stage.

% PICNIC 

PiCnIc (Pipeline for Cancer Inference - \cite{Caravagna2016-vw}) is another pipeline proposed to process multi-omics data. The pipeline has four major components: disease subtyping, event selection, group detection, and model inferences \citep{Caravagna2016-vw}. Each stage in the process has its range of models which makes the software versatile and open to multiple types of data. In the cohort, subtyping aims to find the clusters (through K-means, Gaussian mixtures, etc.) in the given data which are then further processed in the second stage to find the driver genes through the likes of MutSigCV \citep{Lawrence2013-pl}. This is followed by processing the exclusivity of the driver genes applying models such as Dendrix \citep{Vandin2012-cf, Zhao2012-wj}. Lastly, the pipeline is applied to the colorectal cancer cohort from TCGA and validated against known gene pathways which exhibited potential drivers. As iPAC, the work from \cite{Caravagna2016-vw} is a pipeline solution rather then a computational model.

The aim of this section was to give an overview of the work done in combining multiple sources of data, to cover how graphs can be used to analyse the gene network and the suggested workflows in multi-omics. The pipelines covered in this section focused more on analysing the data rather than answering a biological question targeted to disease subtyping or finding potential genes for targeted genes. Drawing on the literature review, it is evident that networks are a popular method for analysing biological data.

\subsection{iCluster family - Matrix factorisation based} \label{s:lit:iCluster}

Matrix factorisation is a widely used mathematical technique for dimension reduction where the data is decomposed into smaller matrices. Processing smaller matrices has a lower computational cost as it can be analysed in parallel. Another reason for dimension reduction, and the main one in computational biology, is that the decomposed matrices embed latent information about the biological data. In other words, by condensing the initial matrix data into smaller matrices, only the relevant information is kept while the noise is discarded. 

iCluster \citep{Shen2009-ew, Mo2013-zi, Mo2018-el} is one class of matrix factorisation along with \acrlong{pca} introduced in \cref{s:lit:dim_red}. iCluster was designed and built with the goal of integrating multiple data types into the latent space as a response to tissue heterogeneity in tumours. The novelty introduced by this family of algorithms is that it supports multiple data-types, unlike PCA. \citet{Shen2009-ew} proposed iCluster, which supports only continuous data. Further work is done by the same group in 2019  where iCluster+ (\citet{Mo2013-zi}) extends the support for binary data. A major critique of those two initial models was the need for parameter fine-tuning and the computational cost to find the latent space. To address this, the group proposed a non-parametric Bayesian model \citet{Mo2018-el} which employs a probabilistic method to find the latent space. 

Even the improved Bayesian iCluster model takes a long time to run, especially when Copy Number Variations is used. There are several applications of iCluster to other cancers including Glioblastoma \citep{Shen2012-yj},  Breast cancer \citep{Curtis2012-ff} and TCGA pan-cancer analysis \citep{Hoadley2018-qe}.

In addition, non-matrix factorisation was used in the referential study of the muscle-invasive bladder cancer by \citet{Robertson2017-mg}. In this case, the authors looked at a non-negative matrix and used a Bayesian approach to find the latent space that is later clustered - a method called Bayesian NMF.

iCluster family is by far the most popular integrative method. It was also used on a pan-cancer \citep{Hoadley2018-qe} study referenced in the MIBC consensus \citep{Kamoun2020-tj}. Compared to other integrative approaches, its main focus is developing a method that helps researchers understand the underlying biology. In terms of limitations, these methods have a high computational cost and are not easy to interpret.

Similarly to the workings of the matrix factorisation algorithms, there are the autoencoders which are trying to find a latent variable space using a more modern approach, namely Deep Learning. Theoretically, these could incorporate multiple data types.

\subsection{Deep Learning  and Genomics} \label{s:lit:dl_genomics}

\vspace{3mm}
\fbox {
    \parbox{\linewidth}{
      \begin{itemize}
        \item Autoencoders used with other methods to predict patient survival 
        \item Autoencoders as a new type of dimension reduction method 
        \item Other deep learning has limited applications to genomics
      \end{itemize}
    }
}
\vspace{3mm}

Deep learning has been the steam engine that fuelled the recent progress in Machine Learning, but these models are typically applied to supervised learning problems and require a large number of samples. In comparison, defining better bladder cancer subtypes from the gene expression, mutations \& other data is an unsupervised learning problem. Current sequencing technologies are not affordable for enabling the creation of large datasets required for DL models. This means that, with the exception of the autoencoders, most of the \acrshort{dl} approaches are not yet suitable for the specific multi-omics problem.

The above statement is supported by a review paper on "The Application of Deep Learning in Cancer Prognosis" by \citet{Zhu2020-cv} where no 'traditional' \acrshort{dl} are used except for the autoencoders in the multi-omics section. Furthermore, the authors provide suggestions to overcome the limitations of \acrshort{dnn} to the cancer research. The ones relevant to this PhD project are data augmentation or imputation, building the models using more domain knowledge, and the use of one-shot techniques. The first suggestion has already been adopted in the field where many researchers have created synthetic data following the distributions of real biological datasets \citep{Zhao2012-wj,Leiserson2015-yk}.

There have been attempts to use \acrshort{dl} to predict survival from omics data, though less successfully. In their model, Cox-nnet \citep{Ching2018-gq}, the authors feed \acrshort{tcga} dataset to an \acrshort{ann} for 10 cancers to predict the survival rate. This achieves reasonable prediction scores but is not significantly better than canonical methods. Despite not being tested on other datasets, \citet{Ching2018-gq} indicates that networks with more layers have the risk of overfitting the dataset.  

% Other applications of DL in genomics
Indeed, most of the \acrshort{dl} paradigms are not directly applicable to the multi-omics problem, but there have been efforts to solve other biological problems. In particular, the AlphaFold model \citep{Jumper2021-du} which predicts protein folding based on the protein sequence with the highest accuracy of a computational model. This is a breakthrough in biology, as the prediction of protein folding has been considered a hard problem to solve.


% The last suggestion, One-Shot learning, is worth considering although in light of the work that has been done by OpenAI it may have some limitations. In their "One-Shot Imitation Learning" paper, \citet{duan2017-ae} use domain randomisation to train their robot to perform a task (e.g., building a tower from different blocks) in a simulated world. The trained model is then used to replicate a similar task in real-life after a human performed a building task that was not present in the training phase. It is challenging to see how this can be applied to a multi-omics problem where the domain expertise is relevant. Some tweaking to the GPT-3\cite{Brown2020-wh} model (from OpenAI) is worth exploring, but that has been used to generate text and focuses on Natural Language Processing problems. A remote application might be to generate synthetic data to test the computational model. Despite the unfavourable prospects, this may represent an exciting venue to follow when there is more progress on this type of model.

\paragraph*{Autoencoders} \label{s:lit:autoencoders}

\citet{Chaudhary2018-qj} have used autoencoders to predict the survival of patients with a particular type of liver cancer, hepatocellular carcinoma (HCC). They used TCGA RNA sequencing, methylation and miRNA data to find correlations between survival and HCC. Their model reduced the number of features to 100 which were later clustered with K-means and used as standard metrics to measure performance with Silhouette and Calinski-Harabasz scores, see \cref{s:lit:clustering_metrics} for more details. The authors show that autoencoders are more successful than PCA in preserving relevant information. However, how PCA was applied to multi-omics data was not described in detail. A criticism of the work from \citeauthor{Chaudhary2018-qj} is that predicting the survival rate is not the most informative method as it does not help to understand how to improve current treatment, while better cancer subtyping does.

Most importantly, Chaudhary confirms that a DL model using multi-omics data yields a better result than one receiving single-omics data. This is further supported by \citet{Ma2019-hk} which introduced another Autoencoder model that integrates multiple data types. These strengthen the project's hypothesis that adding mutation data to gene expression will yield better predictions.

The research effort of \citet{Ma2019-hk} is another example in which autoencoders are applied to the TCGA data. More specifically, the authors used Bladder and Brain Lower Grade Glioma (LGG) on gene expression, miRNA, DNA methylation and protein expression to predict survival events and progression-free intervals. They also have integrated domain knowledge through the molecular interaction network from the STRING database. Their model outperforms the traditional methods and approaches of ML in providing clinical information about tumour events.

Autoencoders have been used to process somatic mutation datasets. In one of the first pieces of work is from \cite{Palazzo2019-hx} using autoencoders to represent the somatic mutations of 40 different types / subtypes of tumours in a lower dimension. The authors acknowledged that their results are promising in preserving biological information in lower dimensions but require further validation.

% is is all very interesting! Can you give specifics, particularly on the bladder work in the previous paragraph - what did they specifically improve, do they link to mRNA subtypes at all etc?

The work mentioned above suggests that autoencoders are a good dimension reduction technique for multi-omics datasets. Their versatility in the input data is one of the advantages along with the reduction of nonlinear patterns. In addition, some of the work suggests that autoencoders preserve biological knowledge, but this needs further validation to support the assumption. As the above discussion indicates, autoencoders were mostly used to predict the survival of a cancer patient and not to refine the cancer subtyping, which is more useful for improving current treatments. Furthermore, multi-omics data was given to the autoencoders and not gene wxpression with mutations, which we know have a strong biological correlation between tumours.

% Summary
\subsection{Summary}

This section provided an overview of the methods used to analyse gene expression, mutations, and integrative approaches. The most popular method for analysing gene expression alone is hierarchical clustering, as used by most of the work in the MIBC consensus \citep{Kamoun2020-tj}. \Cref{s:lit:mutations} surveys the methods used to process mutation data with models such as MDPFinder \citep{Zhao2012-wj} and Dendrix \citep{Vandin2012-cf}. The prevalent approach of these studies is to apply stochastic or greedy methods to identify the subset of relevant mutations. \Cref{s:lit:multi-view} provides an overview of graph/network theory approaches to genomics to identify and weigh the connections between genes. This is followed by \cref{s:pipelines}, which presented proposed pipelines for the analysis of genomic data in which researchers introduce a series of computational steps to apply multiple types of data. Lastly, the application of Deep Learning to genetic data is covered, focussing on autoencoders. A summary of the models reviewed can be seen in Appendix \cref{ap:tables_models} where \cref{tab:data_used} lists the approaches used. In Appendix \cref{tab:approaches}, the same models are examined, but with detailed datasets and the research goals.

From the integrative methods covered in \cref{s:lit:multi-view}, iCluster \citep{Shen2012-yj} is a versatile approach that accepts many types of data and has been widely applied, including the MIBC consensus. However, it is computationally expensive and does not offer a straightforward understanding of the gene contribution to each subtype.

Given the limitation of a small number of samples in biological datasets, traditional Machine Learning methods have had limited success in analysing omics data. Autoencoders might show potential as dimension reduction techniques, but the latent space is harder to interpret than other methods.

The project's goal is to enhance MIBC stratification by integrating multiple data types at the computational stage. The methods identified in the literature survey not only support but also reinforce the promising hypothesis that integrative multi-omics approaches have the potential to unveil new biology. Networks, which are popular in biology for their ability to represent gene relationships and integrate multiple data types, are the chosen computational method for this project. They align with the project's aims and objectives and will be further detailed in the next chapter.