
\subsection{Methods for analysing omics data} \label{s:lit:multi-omics}

\vspace{3mm}
% \noindent\rule{17cm}{0.2pt}
\fbox {
    \parbox{\linewidth}{
      \begin{itemize}
        \item Gives an overview of the methods used in analysing gene expression and mutation data
        \item Integrative approaches used
        \item Covers iClusters and Deep Learning methods
      \end{itemize}
    }
}
\vspace{3mm}

% One of the largest tumour cohort is \acrshort{tcga}\cite{Tcga2018-sj} which consists of roughly 20000 samples across 33 cancer types. An impressive effort, but if one focuses on particular cancer, the available data becomes considerable smaller. For example, the data available for \acrfull{mibc} cancer is 408 samples with a wide-range of molecular information available gene expressions, somatic mutations, copy number variations and metadata \cite{Robertson2017-mg}. This then, becomes a situation of a problem with a low number samples set and large number of features; there are more than 56k genes in a sample, which is usually the other way in the problems addressed in typical ML problems.


% Problems if we just use dim reductions or if we don't take into consideration all aspects of the genes
The stratification of muscle-invasive bladder cancer (MIBC) based on gene expression is characterised by a small number of samples and a large number of features. Previous approaches, such as those included in the MIBC consensus \cite{Kamoun2020-tj}, address this by filtering out unexpressed genes, applying a dimension reduction algorithm, and then performing clustering analysis. This process makes a significant assumption due to the reliance on dimension reduction algorithms which tend to select data points with the highest variance. From an information theory perspective, this assumption holds as relevant information typically indicated by high variance\footnote{That is, the more different a data point is from the average, the more information contains. One can think about this as, if you were to live in the UK, you were more interested when the sunny days are, not when it rains.}. However, genes that commonly drive cancer, such as TP53—which controls cell cycle checkpoints and DNA integrity before cell division—may exhibit high median/average expression levels but not high variance. Analysis of TCGA data for bladder cancer \cite{Tcga2018-sj, Robertson2017-mg} revealed that TP53 expression remains consistent across samples, resulting in low variance. This could indicate that TP53 mutations are common across bladder cancers, as it was presented in the MIBC subtypes summary (\cref{fig:lit:2020_consens}).

Such instances impose a great challenge as the model needs to avoid dismissing genes that do not have a large variance. This issue has been framed as a problem of gene selection in this PhD project and is explored in \cref{s:gene_sel} (gene selection section). Additionally, the project encompasses extensive analysis of various datasets, pursuing objectives beyond disease stratification. Reviewing the advances in analysing biological data from different perspectives assists in selecting the most appropriate methods for the project.


% Later, we will see that this can be measure by the exclusive is the mutations/expression related to the other genes in the set.
The previous work done in the field can be categorised based on the data given to the models. 
It starts with a review of the work done in analysing gene expression data in \ref{s:lit:rnaSeq}. followed by presenting efforts in finding the relevant mutant genes in \ref{s:lit:mutations} and then on the integrative approaches with a focus on iCluster \ref{s:lit:multi-view}. The section briefly touches on the work in genomics which uses Machine Learning (\ref{s:lit:dl_genomics}) and concludes with a critical discussion on which method is the most suitable for the project.

\subsubsection{Gene expression data} \label{s:lit:rnaSeq}

\vspace{3mm}
% \noindent\rule{17cm}{0.2pt}
\fbox {
    \parbox{\linewidth}{
      \begin{itemize}
        \item Introduction of the challenges of working with gene expression
        \item Prevalent approach is hierarchical clustering on RNAseq data
        \item Other clustering techniques
      \end{itemize}
    }
}
\vspace{3mm}

% Talk about how and why unsupervised learning is used for RNAseq.
% Talk about the consensus in the 2020 paper
The proteins are the building blocks of every living organism and implicitly of the tissues. To understand the malfunctioning of the biological processes causing cancer, protein data (proteomics) are the ideal data to analyse malignant behaviour. However, measuring proteins in samples is hard and cost inefficient. As an alternative, RNAseq data is used, which is also increasing more affordable. It is worth mentioning that this measures the abundance (or the transcript) of a gene\footnote{Remember the analogy of the RAM at the start of this Chapter. The DNA is the instruction from hard drive, where the RNA is the instruction from memory and the protein is what the instruction creates.} which is a proxy for the proteins produced. Consequently, it is not a direct measure of the proteins and it might have biological limitations. From an engineering perspective, any additional step in the process is a potential risk to add more errors to the output. Similarly, the RNAseq may not always be an accurate representation of the biological processes. This can be overcome by domain knowledge in the field, and it is where collaboration with JBU plays a vital role.

Genomics datasets (including RNAseq) are characterised by having a small number of data points (tissue samples) but a large number of features (number of genes). Another aspect of the available datasets is that the samples usually come from tumours because obtaining a sample involves surgery, hence there is less publicly available data on healthy tissue. In contrast, as JBU focuses on bladder tissue there is data collected on the urothelium which is the normal tissue counterpart of the cell that is usually transformed in urothelial cancer.

The most popular method to perform clustering analysis is the hierarchical clustering used in the TCGA work \cite{Robertson2017-mg}, consensus \citet{Kamoun2020-tj} and Lund \cite{Sjodahl2017-xr}, explained in detail in \cref{s:lit:subtypes_mibc}. The general working of hierarchical clustering is covered in \cref{s:lit:clustering}. This method is preferred over the more canonical K-means clustering as it can find subtypes of varying sizes as opposed to similar sizes found using K-means. Nevertheless, it is still a parametric solution where the user/researcher needs to make a choice on the number of clusters.

% Talk about sci-fold
More recent work is from \citet{Tian2021-vu} who used the Stiefel manifold method on gene expression, miRNA expression data and DNA methylation. Compared to previous work discussed in this subsection, \citet{Tian2021-vu} focus on four other cohorts from the TCGA: glioblastoma multiforme (GBM), breast invasive carcinoma (BIC) Skim Cutaneous Melanoma (SKCM) and Acute Myeloid Leukemia (AMIL). In their work, the authors suggest that using both gene expression and DNA methylation yields better subtyping but they do not provide clear results on their model.

% other approaches 
The microarrays data is an older method to obtain genomic information from the tissue which, compared to RNAseq, is more prone to error. Some studies take multiple measures of the biopsy, adding a time dimension to the data and making it a good fit for a special type of ANN, Spiking Neural Networks (SNN). \citet{Capecci2020-uj} combined SNN with EA to process microarrays data to classify the gene interactions based on their temporal feature. The study focuses on dermatitis, a skin condition which enables multiple biopsies that give the temporal dimension. This type of sampling is however not possible for most of the diseases, including MIBC.

% Introducing neuroevolution
By combining ANN and EA, \citet{Grisci2019-xn} have introduced a neuroevolution model for analysing microarrays. Their research is built on the FS-NEAT\cite{Whiteson2005-dn} which is an idiosyncratic version of the NEAT model (Neural Networks through Augmenting Topologies)\cite{Stanley2002-tg} focused on the feature selection (FS). The central idea of the NEAT family is to use evolutionary methods to find the optimal initial network structure, which then can be used for classification of the gene expression. This is a different approach from what has been presented so far, but it is not suitable for the project as it relies on knowing the subtypes a priori.

\subsubsection{Gene Mutations} \label{s:lit:mutations}

\vspace{3mm}
\fbox {
    \parbox{\linewidth}{
      \begin{itemize}
        \item Identifying the driver genes is formalised as finding a submatrix of relevant genes 
        \item Dendrix \citet{Vandin2012-cf} from which other work has been derived
        \item MDPFinder (\citet{Zhao2012-wj}) uses EA to find the submatrix of relevant genes
      \end{itemize}
    }
}
\vspace{3mm}


Gene mutations are anomalies that occur at the gene level within the DNA, where one (or more) block (nucleotides) is replaced by the other bases (3 possibilities) or insertion/deletion can happen to the gene. Depending on where they occur in the gene sequence, the mutations have a less negative effect on protein production. This means that simply knowing that a gene is mutated or not, is not enough to correlate to an ill behaviour; instead information about where the mutations happened is required. The mutations that contribute to tumorigenesis are called driver genes and the ones which have a neutral contribution are known as passenger mutations \cite{Ciriello2012-hi}. To retrieve the mutated genes, Whole Exome Sequencing (WES) technique is used to target portions in the gene (exome - more likely to be relevant and cost-effective) and compare it with the reference genome \cite{Schneider2016-ml}. This section covers computational models that seek to find the driver gene in a tissue whereas the project aims to improve bladder cancer classification using mutation data. 

%  dimension reduction, gene subsets, finding the submatrix 
\citet{Vandin2012-cf} determined gene mutation exclusivity by variables and demonstrated that finding the subset (or submatrix) is an NP-hard problem\footnote{ NP stands for Non-deterministic polynomial time and computer scientists describe the NP problem as the most complex problems in terms of the computational cost.}. The selection process of the relevant genes has been demonstrated by the authors to be NP-hard, meaning that it takes a lot of computational resources to solve it due to its complexity. Dendrix, the introduced model, constructs a submatrix with the driver genes of the disease from a larger set of genes. Specifically, it looks at finding the mutated genes that have the following two properties: high coverage (common) and high exclusivity (specific to some samples). The genes that need to be included in the submatrix are chosen by a weighting factor, which takes into account the above two properties as well as the Pearson Correlation of the gene expression. Dendrix provides two solutions: a greedy algorithm (requires a large dataset) and a stochastic one, Monte Carlo. Importantly, Dendrix is a \textit{de novo} algorithm that does not need \textit{a priori} data regarding the gene interactions to determine their significance and it is mathematically rigorous. The authors applied the algorithm to lung adenocarcinoma and glioblastoma multiforme and found known pathways such as \textit{p53} or \textit{Rb pathways}.


% Introducing Comet 
Building on the work of \citet{Vandin2012-cf}, there are several variations of the Dendrix model proposed \cite{Leiserson2013-da,Szczurek2014-dh,Leiserson2015-yk}. Noteworthy is the COMET (\citet{Leiserson2015-yk}) model developed in collaboration with Vandin to overcome the bias introduced by the highly mutated genes to the computational models derived from Dendrix \cite{Vandin2012-ns}. \citet{Leiserson2015-yk} accomplished this by introducing a scoring mechanism that measures how exclusive a mutation is to cancer. This is a more advanced version of the weighting function from the original paper by \citet{Vandin2012-cf} which was used to search for the submatrix of the driver genes. They compared the results with different models\footnote{Multi-Dendrix\cite{Leiserson2013-da} created by the same group and is a variation of the Dendrix (\citet{Vandin2012-cf} model). The other model that they compare with is mutex\cite{Babur2015-qk} which is also looking at the exclusivity of the mutations.} using datasets from TCGA and the COMET outperforms the other models \cite{Leiserson2013-da,Szczurek2014-dh} in terms of finding the genes predominantly mutated in that particular cancer.

% Introducing MDPFinder 
%  The novelty of Zhao et al. is that they take the problem defined by Vadin et al. and replaced the Monte Carlo algorithm with the two other algorithm
\citet{Zhao2012-wj} took a different approach to the submatrix problem formalised by \citet{Vandin2012-cf}. Through their \textit{de novo} model, MDPFinder\cite{Zhao2012-wj}, the authors proposed two solutions to finding the driver mutations genes. The first one is an optimisation algorithm that finds the submatrix of the relevant genes in the carcinogenesis - a method guaranteed to find the solution but dependent on the given data. Importantly, \citet{Zhao2012-wj} found that despite the submatrix problem being an NP-hard problem (\citet{Vandin2012-cf}) it can be solved relatively fast. The second solution proposed was a simple form of \acrfull{ea} which enables a wider search space. In EA terminology, MDPFinder encodes the individuals (potential genes) as strings (of length n, where n is the number of genes). One can see this solution as a method for column selection method applicable to any dataset. The novelty in the evolutionary model comes from the fitness function which accounts for both the mutation information (through the rank of the matrix) and gene expression (via Pearson correlation). This model is applied to multiple datasets (from both TCGA and the ones used with Dendrix\cite{Vandin2012-cf}) and all the models\footnote{Remember that \citet{Vandin2012-cf} proposed two algorithms the stochastic and greedy algorithm} yield the same results, but \citet{Zhao2012-wj} have a better computational time\footnote{The scale is in seconds}. It is worth emphasising that \citet{Zhao2012-wj} uses \acrshort{ea} for their ability to widely search the solution space rather than the white-box characteristics. The authors applied the algorithm to head and neck squamous cell carcinoma, glioblastoma and ovarian cancer, and it revealed subsets of gens with biological functions.

There is no consensus on how the mutation data is processed, as in the case with RNASeq where hierarchical clustering was the canonical approach. However, it seems that in these datasets a wider range of algorithms has been applied with the work of \citet{Zhao2012-wj} (MDPFinder) which can be seen as an early effort to apply \acrshort{ea} to genomics. In addition, \citet{Vandin2012-cf} mathematically formalise the search for driver mutations which can help the computational side. MDPFinder and Dendrix prove to be two relevant models for analysing mutations. 

Whereas most of the research efforts presented here differs in terms of aims from the current project, \citet{Zhao2012-wj} share some similarities. Instead of basing the method on the gene expression, \citet{Zhao2012-wj} first analyse the mutations and then use the gene expression. In this thesis, a representation is first built on the gene expression and then the mutation information is added. The network approach presented in this thesis offers an easier option to integrate other data types such as Transcription Factors.


\subsubsection{Integrative approaches} \label{s:lit:multi-view}

\vspace{3mm}
\fbox {
    \parbox{\linewidth}{
      \begin{itemize}
        \item Graph/Network theory is the prevalent method
        \item iCluster used for data integration based on NMF 
        \item There are some efforts to integrate multiple steps in a flow
      \end{itemize}
    }
}
\vspace{3mm}

% introducing graphs and how they may be used
In previous sections, mutation data is analysed with the central goal to find the submatrix that represents the driver genes. It can be stated that it represents a linear algebraic method to genetics, but it is not the only one present. Analysing the data through the lens of a network/graph problem is a widely used method to understand the gene relationship. The end goal of analysing the genomic data is to find and understand the interactions in the gene network and its perturbation in the healthy tissue since this is more likely to lead to treatment solutions. Specifically, researchers are interested in the mutated genes that influence the network. Therefore, graph theory is essential for processing this data. As we have seen in \cref{s:lit:graph_overview}, in the context of genomic data, nodes are represented by genes and edges are the connections between them. The graph theory aims to highlight the nodes with important connections that drive the network. The significance of a connection is given by a weighting factor.

% MEMO
MeMo (Mutual exclusivity model)\cite{Ciriello2012-hi} is a graph model that uses networks and community detection for finding the exclusive groups of mutated genes as well as different statistical methods. The model integrates gene mutations, copy number alterations and expression into a network representation which is then analysed to find similar groups of genes. Their approach is familiar to the research efforts covered in \cref{s:lit:mutations} as the authors first build a binary matrix based on the altered genes, followed by a graph representation which is then correlated with some pathway information. Finally, an exclusivity algorithm is applied with similar aims to those of the Dendrix model by \citet{Vandin2012-cf}. \citet{Ciriello2012-hi} applied the model to the glioblastoma and breast cancer TCGA cohorts and were able to validate against known mutated genes such as TP53 but also showed potential new biology. MeMo uses Jaccard coefficient to build the network compared to the correlation metrics used in the networks approaches explored in this project. 

% DriverNet and DawnRank.
Using graphs combined with transcriptomic, mutation and information, DriverNet is proposed by \citet{Bashashati2012-lk}, a model which creates a bipartite graph. The network is created on the information known from protein-protein interaction networks to which multiple data types are applied using an influence graph algorithm developed by \citet{Vandin2011-bs}\footnote{Note this is the same first author who developed the Dendrix model and the work is presented in \cref{s:lit:mutations}}. DriverNet processes these edges through a greedy optimisation approach to find the potential driver genes based on the connection between the two layers of information. The work is applied to four cancers and some of the known important genes, such as \textit{ERBB2} and \textit{EGFR}, are detected in the glioblastoma and breast cancer. The research effort from \citet{Bashashati2012-lk} shows another network method which can enable the integration of multiple datasets.

DriverNet was derived by \citet{Hou2014-se} who introduced DawnRank, a model that helps to identify driver genes in a sample and not across a population as the previous models do. The authors combine DriverNet with PageRank to find the driver genes; this is the famous algorithm upon which Google was built. \citet{Hou2014-se} build the gene interaction network with MeMo \cite{Ciriello2012-hi} which uses mutations, expression and copy number alterations data. DawnRank uses both tumorous and healthy samples from the donor which enables scientists to find individual genetic differences specific to the tumour, not the individual. \citet{Hou2014-se} consider that the DawnRank's main limitations are that it is dependent on building the molecular interaction network which is usually incomplete and is neither cancer nor patient specific. 


Finding how genes are related to each other and if they are part of a similar pathway is a common analysis in genetics. The most popular method is Gene Set Enrichment Analysis (GSEA) which receives a rank list of genes and then performs a wide search. There are other methods with simillar aims GANPA (Gene Association Network-based Pathway Analysis) \cite{Fang2012-vr}, LEGO (Link Enrichment of Gene Ontology) \cite{Dong2016-zs} or the work from \citet{Cava2018-rv}. The network representation is important as it helps to determine the distance or proximity of the genes on a pathway. These can also be seen as a more advanced method to determine the central (or most important) nodes in a network, but relies on the gene pathways information, which is incomplete.

A herculean effort by \citet{Xie2021-al} was to quantify and review the work done in the field of Gene Set Analysis. They reviewed 504 papers and looked at their performance based on citations, how they correlate to different approaches, and how the models have been benchmarked. In their review, the authors proposed a set of criteria that increases the quality of the research produced in bioinformatics but also suggested embedding more domain knowledge. Most of them are similar to the guiding principles in Machine Learning, defined by using benchmarks, publishing the code as open-source, and using data available.

% LEGO, GANPA and Cava et al.
% When the gene network interaction is seen as a graph, the connections of a gene to the other is relevant as well as the individual weight of each edge. Driver genes may be found by the number and strength of its connections. Finding this is formally known in graph theory as analysing the degree of centrality of a node. Solutions to this problem are GANPA ("Gene Association Network-based Pathway Analysis") \cite{Fang2012-vr} and LEGO ("functional Link Enrichment of Gene Ontology or gene sets") \cite{Dong2016-zs} which aim to find the relevant genes in the network. The main difference between these two is that \citet{Fang2012-vr}propose a solution to a problem classified as Functional Category Score (FCS) while \cite{Dong2016-zs} solve Over Representation Analysis (ORA). 

% \citet{Cava2018-rv} build on LEGO/GANPA to integrate multiplex networks with the goal of the network drivers in pan-cancer. The authors compare their model with some already mentioned above MDPFinder \cite{Zhao2012-wj}, Dendrix\cite{Vandin2012-cf}, DriverNet \cite{Bashashati2012-lk}, DawnRank \cite{Hou2014-se} and others (see tables from Appendix \cref{ap:tables_models}). It can be noticed that all three models LEGO, GANPA and \citet{Cava2018-rv} are using pathway, networks and expression information but not mutations.

% "Gene Association Network-based Pathway Analysis" (GANPA) \cite{Fang2012-vr} is a method that determines the weighting in signalling pathways by considering the microarrays data. A similar approach was developed by Dong et al. \cite{Dong2016-zs} called LEGO ("functional Link Enrichment of Gene Ontology or gene sets"). The main difference between these two is that Fang et al. propose a solution to a problem classified as Functional Category Score (FCS) while Dong et al. solve Over Representation Analysis (ORA). The actual difference seems to be related to the gene of interest but I don't understand it as I haven't considered it important to follow up. However, the importance of these two models comes from their common graph approach and that is of analysing the degree of centrality of a gene. This means how many connections the genes have and what's their weight. Processing this gives information about how relevant can be.

% GANPA/LEGO is using data on gene expression (microarrays) or network pathways (KEGG). 
% Cava et al. \cite{Cava2018-rv} builds on LEGO/GANPA to integrate multiplex networks with the goal of the network drivers in pan-cancer. 


\subsubsection*{Existing pipelines to integrate multiple datasets} \label{s:pipelines}


% Related to GANPA and Cava - GeneSetCluster
Within the same class as GSEA, GANPA\cite{Fan2020-yb} and LEGO\cite{Dong2016-zs}, \citet{Ewing2020-os} introduced a pipeline (GeneSetCluster) to process the gene sets yielded by different sequencing techniques. GeneSetCluster consists of three stages: pre-processing (harmonise in the paper), computing the distances between genes and clustering (k-means and hierarchical clustering). The authors do not propose a new integrative method to find potentially relevant genes in the dataset, but propose a general pipeline solution that groups the output of gene set analysis (e.g. GSEA) for multiple data types.

% Multi-genetics approach, something built on Neuroevolution
\citet{Feltes2020-bz} perform a thorough pan-cancer analysis of the gene expression on a large number of datasets. After the initial preprocessing stage, the authors narrowed it down to only 82 microarray studies and 17 RNAseq datasets. They applied the Neuroevolution model\cite{Grisci2019-xn} which was based on FS-NEAT\cite{Whiteson2005-dn} (see \cref{s:lit:rnaSeq}). The last step was to use STRING 11\cite{Szklarczyk2019-pu} metasearch to build a Protein-Protein network. One interesting conclusion in the study from \citet{Feltes2020-bz} was that underexpressed genes in the bladder are similar to the ones in breast, colorectal, lung, and ovarian cancer. Moreover, the study suggests that there is a stronger correlation between the carcinogenic character of over-expressed genes compared to an under-expressed one. This further enforces that integrative approaches form a better representation of the biology.

% iPAC
\citet{Aure2013-je} introduce multiple statistical tools that correlate the copy number alterations with the expression of genes through a pipeline called iPAC (in-trans process associated and ciscorrelated) and further refined their results via Gene Ontology \cite{Carbon2018-ah}. The authors applied their method to breast cancer which is capable to find known driver genes but potential candidates. iPAC is a statistical method focusing on the disruption of the epigenetic machinery with potential applications to MIBC, but it does not consider other relevant information such as gene expression or mutations at the computational stage.

% PICNIC 

PiCnIc (Pipeline for Cancer Inference) \cite{Caravagna2016-vw} is another pipeline proposed to process multi-omics data. The pipeline has four major components: disease subtyping, events selection, groups detection and model inferences\cite{Caravagna2016-vw}. Each stage in the process has its range of models which makes the software versatile and open to multiple types of data. In the cohort, subtyping aims to find the clusters (via k-means, Gaussian mixtures, etc.) in the given data which are then further processed in the second stage to find the driver genes through the likes of MutSigCV \cite{Lawrence2013-pl}. This is followed by processing the exclusivity of the driver genes applying models such as Dendrix \cite{Vandin2012-cf}, \cite{Zhao2012-wj}. Lastly, the pipeline is applied to the colorectal cancer cohort from TCGA and validated against known gene pathways which exhibited potential drivers. As iPAC, the work from \cite{Caravagna2016-vw} is a pipeline solution rather then a computational model.

The aim of this section was to give an overview of the work done in combining multiple sources of data, to cover how graphs can be used to analyse the gene network and the suggested workflows in multi-omics. The Pipelines covered in this section focused more on analysing the data rather than answering a biological question targeted to disease subtyping or finding potential genes for targeted genes. Drawing on the literature review, it is evident that networks are a popular method for analysing biological data.

\subsubsection{iCluster family - Matrix factorisation based} \label{s:lit:iCluster}

Matrix factorisation is a widely used mathematical technique for dimension reduction where the data is decomposed into smaller matrices. Processing smaller matrices has a lower computational cost as it can be analysed in parallel. Another reason for dimension reduction, and the main one in computational biology, is that the decomposed matrices embed latent information about the biological data. In other words, by condensing the initial matrix data into smaller matrices only the relevant information is kept while the noise is discarded. 

iCluster \cite{Shen2009-ew, Mo2013-zi, Mo2018-el} is one class of matrix factorisation alongside \acrlong{pca}. iCluster was designed and built with the goal of integrating multiple data types into the latent space as a response to tissue heterogeneity in tumours. The novelty introduced by this family of algorithms is that it supports multiple types of data-types, unlike PCA. The \citet{Shen2009-ew} proposed iCluster, which supports only continuous data. Further work is done by the same group in 2019  where iCluster+ (\citet{Mo2013-zi}) extends the support for binary data. A major critique of those two initial models was the need for parameter fine-tuning and the computational cost to find the latent space. To address this, the group proposed a non-parametric Bayesian model \citet{Mo2018-el} which employs a probabilistic method to find the latent space. 

Even the improved Bayesian iCluster model takes a long time to run, especially when Copy Number Variations is used\footnote{The Bayesian iCluster was run with gene expression, mutations and copy number variations in this project.}.There are several applications of iCluster to other cancers including Glioblastoma \cite{Shen2012-yj},  Breast cancer \citet{Curtis2012-ff} and the TCGA pan-cancer analysis (\citet{Hoadley2018-qe}).

In addition, non-matrix factorisation was used in the referential study of the muscle-invasive bladder cancer by \citet{Robertson2017-mg}. In this case, the authors looked at a non-negative matrix and used a Bayesian approach to find the latent space that is later clustered - a method called Bayesian NMF.

iCluster family is by far the most popular integrative method. It was also used on a pan-cancer \cite{Hoadley2018-qe} study referenced in the MIBC consensus \citet{Kamoun2020-tj}. Compared to other integrative approaches, its main drive is to develop a method that helps the researchers to understand the underlying biology. In terms of limitations, these methods have a high computational cost and are not easy to interpret.

Similarly to the workings of the matrix factorisation algorithms, there are the Autoencoders which are trying to find a latent variable space using a more modern approach, namely Deep Learning. Theoretically, these could incorporate multiple data types.

\subsubsection{Deep Learning  and Genomics} \label{s:lit:dl_genomics}

\vspace{3mm}
\fbox {
    \parbox{\linewidth}{
      \begin{itemize}
        \item Autoencoders used with other methods to predict patient survival 
        \item Autoencoders as a new type of dimension reduction method 
        \item Other deep learning has limited applications to genomics
      \end{itemize}
    }
}
\vspace{3mm}

Deep Learning has been the steam engine that fueled the recent progress in Machine Learning, but these models are typically applied to supervised learning problems and require a large number of samples. In comparison, defining better bladder cancer subtypes from the Gene Expression, mutations \& other data is an unsupervised learning problem. The current sequencing technologies are not affordable so as to enable large datasets. This means that, with the exception of the Autoencoders, most of the \acrshort{dl} approaches are not yet suitable for the specific multi-omics problem.

The above statement is further supported by a review paper on "The Application of Deep Learning in Cancer Prognosis" (2020) by \citet{Zhu2020-cv} where no 'traditional' \acrshort{dl} are used except for the Autoencoders in the multi-omics section. Furthermore, the authors provide suggestions to overcome the limitations of \acrshort{dnn} to the cancer research. The ones relevant to this PhD project are data augmentation or imputation, building the models using more domain knowledge, and the use of one-shot techniques. The first suggestion has already been adopted in the field where many researchers have created synthetic data \cite{Zhao2012-wj,Leiserson2015-yk} following the distributions from real biological datasets.

The last suggestion, One-Shot learning, is worth considering although in light of the work that has been done by OpenAI it may have some limitations. In their "One-Shot Imitation Learning" paper, \citet{duan2017-ae} use domain randomisation to train their robot to perform a task (e.g., building a tower from different blocks) in a simulated world. The trained model is then used to replicate a similar task in real-life after a human performed a building task that was not present in the training phase. It is challenging to see how this can be applied to a multi-omics problem where the domain expertise is relevant. Some tweaking to the GPT-3\cite{Brown2020-wh} model (from OpenAI) is worth exploring, but that has been used to generate text and focuses on Natural Language Processing problems. A remote application might be to generate synthetic data to test the computational model. Despite the unfavourable prospects, this may represent an exciting venue to follow when there is more progress on this type of model.

There have been attempts to use \acrshort{dl} to predict survival from omics data though less successly. In their model, Cox-nnet \cite{Ching2018-gq}, the authors feed \acrshort{tcga} dataset to an \acrshort{ann} for 10 cancers to predict the survival rate. This achieves reasonable prediction scores but is not significantly better than the canonical methods. Despite not being tested on other datasets, \citet{Ching2018-gq} indicate that Networks with more layers have the risk of overfitting the dataset.  


% Other applications of DL in genomics
Indeed, most of the \acrshort{dl} paradigms are not directly applicable to the multi-omics problem, but there have been efforts in solving other biological problems. Most notably, the AlphaFold model\cite{Jumper2021-du} (DeepMind) which predicts the protein folding based on the protein sequence with the highest accuracy of a computational model. This is a breakthrough in biology as predicting protein folding has been considered a hard problem to solve.


\paragraph*{Autoencoders} \label{s:lit:autoencoders}

\citet{Chaudhary2018-qj} have used Autoencoders to predict the survival of patients from a particular type of liver cancer, hepatocellular carcinoma (HCC). They have used the RNA-seq, methylation and miRNA data from TCGA to find the correlations between the survival and HCC. Their model reduced the number of features to 100 which were later clustered with K-means and used as the standard metrics to measure the performance with Silhouette and Calinski-Harabasz scores (see \cref{s:lit:clustering_metrics} for more details). The authors show that the autoencoders are  more successful than the PCA at preserving the relevant information. However, how PCA was applied to multi-omics data was not described in detail. A criticism of the work of \citet{Chaudhary2018-qj} is that predicting survival rate is not the most informative method as it does not help understand how to improve the current treatment whereas better cancer subtyping does.

Most importantly, Chaudhary confirms that a DL model using multi-omics data yields a better result than one receiving single-omics data. This is further supported by \citet{Ma2019-hk} which introduced another Autoencoder model that integrates multiple data types. These strengthen the project's hypothesis that adding mutation data to gene expression will yield better predictions.

The work of \citet{Ma2019-hk} is another instance where Autoencoders are applied to TCGA data. More specifically, the authors used Bladder and Brain Lower Grade Glioma (LGG) on gene expression, miRNA, DNA methylation and protein expression to predict survival events and progression-free intervals. They have also integrated domain knowledge through the molecular interaction network from the STRING database. Their model outperforms the traditional methods and approaches of ML in providing clinical information about tumour events.

Autoencoders have been used for processing somatic mutation datasets. In one of the first pieces of work in this realm, Palazzo et al. \cite{Palazzo2019-hx} use AutoEncoders to represent the somatic mutations from 40 different tumour types/subtypes in lower dimension. The authors acknowledged that their results are promising to preserve biological information in lower dimensions but require further validation.

% is is all very interesting! Can you give specifics, particularly on the bladder work in the previous paragraph - what did they specifically improve, do they link to mRNA subtypes at all etc?

The work mentioned above suggests that Autoencoders are a good dimension reduction technique for multi-omics datasets. Their versatility on the input data is one of the advantages along with reducing nonlinear patterns. In addition, some of the work suggests that Autoencoders preserve biological knowledge, but this needs further validation to support the assumption. As the above discussion indicates, Autoencoders were mostly used to predict the survival of a cancer patient and not to refine the cancer subtyping which is more useful to improve the current treatments. Furthermore, multi-omics data was given to the Autoencoders and not Gene Expression with mutations, which we know have a strong biological correlation between tumours.

% Summary
\subsubsection{Summary}

The purpose of this section has been to provide an overview of the methods used to analyse gene expression, mutations, and integrative approaches. For analysing gene expression alone, the most popular method is hierarchical clustering, as used by most of the work in the MIBC consensus \cite{Kamoun2020-tj}. \Cref{s:lit:mutations} surveys the methods used in processing mutation data with models such as MDPFinder \cite{Zhao2012-wj} and Dendrix \cite{Vandin2012-cf}. The prevalent approach in these studies is to apply stochastic or greedy methods to identify the subset of relevant mutations. \Cref{s:lit:multi-view} provides an overview of graph/network theory approaches to genomics with the aim of identifying and weighing the connections between genes. This is followed by \cref{s:pipelines}, which catalogues proposed pipelines for analysing genomic data whereby researchers introduce a series of computational steps to apply multiple data types. Lastly, the application of Deep Learning to genetic data is covered, with a focus on Autoencoders. A summary of the models reviewed can be seen in Appendix \cref{ap:tables_models} where \cref{tab:data_used} lists the approaches used. In Appendix \cref{tab:approaches}, the same models are examined, but with the datasets and goals of the research detailed.

From the integrative methods covered in \cref{s:lit:multi-view}, iCluster \cite{Shen2012-yj} is a versatile approach that accepts a large number of data types and has been widely applied, including in the MIBC consensus. However, it is computationally expensive and does not offer a straightforward way to understand the gene contribution to each subtype.

Given the limitation of a small number of samples in biological datasets, traditional Machine Learning methods have had limited success in analysing omics data. Autoencoders might show potential as dimension reduction techniques, but the latent space is harder to interpret than that of other methods.

The project aims to improve MIBC stratification by incorporating multiple data types at the computational stage. The methods found in the literature survey reinforce the hypothesis that integrative multi-omics approaches have the potential to reveal new biology. Networks are popular in biology as they can represent relationships between genes and integrate multiple data types. Graphs are also intuitive to understand and do not require extensive computational resources. Based on this, networks are the appropriate computational method for this project as they meet all the aims and objectives.