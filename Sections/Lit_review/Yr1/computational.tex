\subsection{Computational methods} \label{s:lit:computational}

\vspace{3mm}
% \noindent\rule{17cm}{0.2pt}
\fbox {
    \parbox{\linewidth}{
      \begin{itemize}
        \item What are the computational methods used for analysing data
        \item EA, ANN
        \item Networks
        \item What method is the most suitable forthe project
      \end{itemize}
    }
}
\vspace{3mm}

\acrfull{ml} and \acrfull{ai} have become buzz words in the past 10 years which have largely been due to the breakthroughs in difficult problems like Computer Vision\cite{Krizhevsky2012-qa} coupled with the increase in computational power. There is usually confusion between the ML \& AI terms, the former represents a subfield of the AI \cite{Domingos_Pedro2015-xr}. In the "Master Algorithm", \citet{Domingos_Pedro2015-xr} defines that the goal of the AI field is to make computers learn better than humans. This has been turned in popular culture to have three milestones, turned into three stages of AI: narrow, human-level and general AI. Currently, we don't have anything remotely close to human-level and arguably we are a long way\footnote{See paper "Artificial intelligence hits the barrier of meaning" by \citet{Mitchell2019-hv}}. For example, in Computer Vision, the models can detect complex patterns and score high in classifications problems but the algorithm doesn't create any meaning or complex representation of the object classified. These limitations can be seen in the failed promises from Tesla, Waymo and other companies seeking to achieve full self-driving cars\footnote{Dr F. Piekniewski is an AI researcher who has a more cynical view of the AI progress and the big claims some popular tech figures do. Specifically, on self-driving cars, its blog post is interesting \cite{Piekniewski2021-if}.}.  Similarly, for the Large Language Models (LLMs) like ChatGPT or Bard which massively advanced the chatbots but the models still can not form an understanding of the real world. Thus, one can see the AI term as more of an aspiration to which the field aims.

Machine Learning represents the collection of tools that are used in Artificial Intelligence development. According to \citet{Domingos_Pedro2015-xr}, there are five schools of thought in this area: the Symbolists, Analogizers, Bayesian, Connectionists, and Evolutionary. Symbolists are looking at drawing knowledge from logic symbols while the Analogizers are extrapolating information from mathematics and logic\cite{Domingos_Pedro2015-xr}. Connectionists and Evolutionary approaches take inspiration from biology, the former from the brain and the latter from Darwinian evolution. Bayesians are concerned with the uncertainty and are dealing with that probabilistic inference through Bayes's theorem\cite{Domingos_Pedro2015-xr}. As the readers will see in the following chapters most of the current work in bioinformatics is using Connectionists, Bayesian and sometimes Evolutionary approaches.

From an ML stance, learning can be of three types: supervised, semi-supervised and unsupervised learning. The first case is when the human labels the data, the corollary being that there is a need for careful processing as well as already having information about the data. This is the 'easiest' case as it comes with a wealth of information and the output is known to belong to the pre-defined set of labels. In this are the most recent progress in \acrfull{dl} was made but it's not a realistic scenario as labelling data is usually challenging and expensive. The semi-supervised (or \acrfull{rl}) is when the model is not given labelled data to learn from, but a set of rules from where it needs to find the solution. This approach has met some successes through \acrfull{dqn} which achieved human skills at Atari games\cite{Mnih2015-cw} or AlphaGo Zero\cite{Silver2017-sw} which is the best Go player in the world. 

As the name suggests the unsupervised learning is the case where there is no input from a human. These computational approaches are used when there is not enough data about the problem, what to expect and patterns are hard to define. Clustering is the prevalent algorithm, that is to find patterns in data which then can be validated with domain knowledge. This is the common scenario in biological applications that data is not labelled, unsupervised approaches are used as seen in \cref{s:lit:rnaSeq}.


\textbf{Revise this after structuring}. The Chapter starts by covering the clustering techniques (\ref{s:lit:clustering}) to support the concepts in consensus subtyping (\ref{s:lit:rnaSeq}). Popular dimension reduction algorithms are widely in genomics where there is a high number of features and are covered in section \ref{s:lit:dim_red}. This is followed by EA (\cref{s:lit:ea_overview}) and Graph Theory (section \cref{s:lit:graph_overview}) basics in order to support some of the work presented in \cref{s:lit:mutations} and \ref{s:lit:multi-view}. Next \acrlong{ann} are introduced covering concepts like Autoencoders or Spiking Neural Networks, the first is needed for \cref{s:lit:autoencoders}. Lastly, the Neuroevolution \cref{s:lit:neuroevolution} introduces concepts on how to combine the evolutionary with the connectionist approach. 

\subsubsection{Clustering} \label{s:lit:clustering}

Clustering algorithms have many variations and these are best covered in a code sample \cite{Scikit-learn_undated-ax} from the Scikit-learn library \cite{Pedregosa2011-ts}. A selection of the methods which worked on this project are displayed in \cref{fig:clustering_types}. The rows represent different types of datasets and the columns are the algorithms covered in this document: K-means being a general-purpose algorithm, Ward and Agglomerative both being hierarchical clustering, but Agglomerative is peforming pair-wise grouping. It is worth mentioning, that a modified version of the Scikit-learn code was adapted to the project needs, enabling the run pf multiples clustering techniques with varied parameters to the JBU and TCGA datasets. 

\begin{figure}[!htb]
  \centering\includegraphics[width=0.8\textwidth,height=0.5\textheight,keepaspectratio]{Images/Clustering/scikit_selected.png}
    \caption{How the algorithms covered in this document behave with different datasets (rows). Image adapted from \cite{Scikit-learn_undated-ax}}
    \label{fig:clustering_types}
\end{figure}
\FloatBarrier

One of the most popular methods (and the simplest) is K-means clustering which tries to find patterns in the data by grouping the data points by distance. There are variations of the algorithms where the datasets are split into multiple batches to improve performances (see \textit{MiniBatch Kmeans} from  \cref{fig:clustering_types}); or Fuzzy K-means that output the cluster membership of each data point. This means, that apart from the cluster labelling there is additional information about how close each point is to a cluster\footnote{For example, if there are 3 clusters, a data point might be 90\% in cluster 1, 6\% in cluster 2 and 4\% in cluster 3.}. From the below K-means pseudocode\footnote{Pseuscode is an accessible method to describe an algorithm.} (\cref{code:k-means}) a few things are worth emphasizing:

\begin{itemize}
  \item The number of centroids (K) is defined by the user.
  \item The distance between two points can be of different types. The euclidean is common, but for higher-dimension cosine is more suitable.
  \item Even though the centroids are randomly initialised new values is computed at each step by averaging the distance of the points in that cluster to the old centroid.
  \item The algorithm converged when the centroids don't significantly change.
\end{itemize}

\begin{lstlisting}[caption={K-means pseudocode}, label={code:k-means}]
  Initialise the centroids at random positions
  while not converged 
    For each data point
      Compute the distance to all the centroids
      The closest represents the cluster to which the data point belongs
    Update the centroids based on the mean distance of each cluster
\end{lstlisting} 

Agglomerative clustering is a type of hierarchical clustering that starts from considering each data point to be its cluster, then, computes higher up groups based on the given linkage method. These algorithms (pseudocode in \cref{code:agg_clustering}) build hierarchical trees and can be seen visually in dendrogram figures which are useful in visualising the clustering evolution. Both K-means and Agglomerative Clustering can use different types of distances, but the latter does not require to set the number of centroids, but the linkage needs to be set. This setting configures the algorithm how the datapoint grouping is performed and the Scikit-learn supports the following\footnote{
  There is a nice visualisation for each of these hierarchical clustering in this \href{https://towardsdatascience.com/machine-learning-algorithms-part-12-hierarchical-agglomerative-clustering-example-in-python-1e18e0075019}{Medium post} }:
\begin{itemize}
  \item Average - Grouping is done by the average distance between cluster points.
  \item Ward - Merging clusters by the sum squared distances. This linkage minimizes the variance and it's similar to K-means.
  \item Single - The distance between two groups is given by the two closest points. Think of this as considering clusters to be merged that have the closest point.
  \item Complete - The opposite to Single linkage, the distance between the two groups is given by the farthest points. In this case, we look at the outer layer points and it may give a more accurate grouping.
\end{itemize}


\begin{lstlisting}[caption={Agglomerative hierarchical clustering pseudocode, style=\small}, label={code:agg_clustering}]
  To each data point assign a cluster number
  while more than one cluster
    Group the closest datapoints together 
    Smaller clusters morph together into a larger ones
\end{lstlisting} 

\subsubsection*{Clustering metrics} \label{s:lit:clustering_metrics}

One of the challenges in clustering is to measure the performance of grouping as there is no labelling or prior information on how the grouping should look like. This project uses the canonical metrics which are supported by Scikit-learn\cite{Pedregosa2011-ts,Scikit-learn_undated-ax}. These are:
\begin{itemize}
  \item Silhouette Coefficient - higher the better. A higher Silhouette Coefficient score relates to a model with better-defined clusters. 
  % It can use different distances and the score is calculated per sample as it follows:
  % \begin{itemize}
  %   \item The mean distance between a sample and all other points in a class.
  %   \item The mean distance between a sample and all other points in the next nearest cluster.
  %   \item The total score for the dataset is the mean of the above two points.
  % \end{itemize}
  \item Calinski-Harabasz Index - higher the better. A higher Calinski-Harabasz score relates to a model with better-defined clusters.  The score is the index is the ratio of the sum of between-clusters dispersion and of inter-cluster dispersion for all clusters (where dispersion is defined as the sum of distances squared).
  \item Davies-Bouldin Index - lower the better. A lower Davies-Bouldin index relates to a model with better separation between the clusters. The index is the average ‘similarity’ between clusters, where the similarity is a measure that compares the distance between clusters with the size of the clusters themselves.
\end{itemize}


In addition to the above metrics, there is a popular heuristic procedure, the Elbow method, which aids in choosing the right number of clusters. In \cref{fig:elbow_method} the y-axis is represented by the distance of the points from their centroids, while the x-axis is the number of clusters. It can be seen that there is a point of inflexion when the number of clusters is set to five and that is considered to be the optimal number of clusters.

\begin{figure}[!htb]
  \centering\includegraphics[width=0.9\textwidth,height=0.5\textheight,keepaspectratio]{Images/Clustering/elbow_method.png}
    \caption{Example of the elbow method, where 5 clusters are the optimal number of clusters. This has been applied on unplubished data of Bening Uropathies from Jack Birch Unit. The green line is the computational time. }
    \label{fig:elbow_method}
\end{figure}
\FloatBarrier


\subsubsection{Dimension reduction} \label{s:lit:dim_red}

In contrast to many computationally- or physics-focused data analysis problems, biologically derived datasets are characterised by a high number of features (dimension) and a small number of samples. Thus, it is often essential to use dimension reduction, \acrfull{pca} is the standard method that works by projecting the higher dimension to the specified lower dimension. For example, a principal component of a genomic dataset might be linked to biological sex, as it influences many features of a biological system.

However, this doesn't accommodate well with non-linear patterns for which \acrfull{umap} and \acrfull{tsne} are used. Both are stochastic and non-linear dimension reduction techniques widely used in bioinformatics. UMAP is gaining popularity as it's been more stable in representing both within and between cluster sample relatedness.

Another dimension reduction technique is \acrfull{nmf} which operates on the same principles as PCA which is by finding a matrix of a lower rank with the minimum information loss. This means that the features present in the data are preserved better when reduced to lower dimensions while the ones that do not contribute to the data resolution are discarded. In addition, NMF conserves the non-linear aspects of the data and a Bayesian version was used in the \acrfull{tcga} classification by \citet{Robertson2017-mg}. NMF also were used in the works of propagating (mutation) data into the networks by \citet{Yang2016-dm, Cai2008-fv} and covered in \cref{s:lit:net_prop}.


% s is a nice section summary style paragraph - "look there are these methods with different strengths, so need to choose carefully, but sometimes we can combine". I would maybe rename this "Choosing the correct ML approach" (or similar) and use it to round off the whole section and link into the next


% \import{./}{general_ml.tex}

% \import{./}{ea.tex}

% \import{./}{snn.tex}